{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from mlflow.tracking import MlflowClient\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "# statistics\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"/workspace/Storage/template_structured/Data/raw/train.csv\"\n",
    "path_test = \"/workspace/Storage/template_structured/Data/raw/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path_train)\n",
    "test = pd.read_csv(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['UID', '주거 형태', '현재 직장 근속 연수', '대출 목적', '대출 상환 기간']:\n",
    "    train[col] = train[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train, test_size=0.4, random_state=421)\n",
    "val, test = train_test_split(val, test_size=0.5, random_state=421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"채무 불이행 여부\"\n",
    "X_train, y_train = train[[x for x in train.columns if x != target]], train[target]\n",
    "X_val, y_val = val[[x for x in val.columns if x != target]], val[target]\n",
    "X_test, y_test = test[[x for x in test.columns if x != target]], test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 콜백: epoch별 메트릭 로그 기록\n",
    "\n",
    "class MlflowLoggingCallbackLGBM:\n",
    "    def __init__(self, X_test, y_test, metric):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.metric = metric\n",
    "\n",
    "    def __call__(self, env):\n",
    "        # 매 step 마다 호출\n",
    "        iteration = env.iteration + 1  # 0-indexed\n",
    "        y_pred = env.model.predict(self.X_test, num_iteration=iteration)\n",
    "        score = self.metric(self.y_test, y_pred)\n",
    "\n",
    "        # MLflow에 기록\n",
    "        # 검증 세트 점수도 기록\n",
    "        for  valid_name, metric_name, valid_result, _ in env.evaluation_result_list:\n",
    "            metrics = {\n",
    "                \"test\" : score,\n",
    "                \"valid\" : valid_result\n",
    "            }\n",
    "        mlflow.log_metrics(metrics, step=iteration, synchronous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MlflowLoggingCallbackLGBM:\n",
    "    def __init__(self, X_test, y_test, metric):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.metric = metric\n",
    "\n",
    "    def __call__(self, env):\n",
    "        # 매 step 마다 호출\n",
    "        iteration = env.iteration + 1  # 0-indexed\n",
    "        y_pred = env.model.predict(self.X_test, num_iteration=iteration)\n",
    "        score = self.metric(self.y_test, y_pred)\n",
    "\n",
    "        # 손실(Loss) 값 가져오기\n",
    "        \n",
    "        metrics = {\n",
    "            \"test\": score,\n",
    "        }\n",
    "        # MLflow에 기록\n",
    "        for name, metric_name, value, _ in env.evaluation_result_list:\n",
    "            metrics[name] = value\n",
    "        mlflow.log_metrics(metrics, step=iteration, synchronous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MlflowLoggingCallbackLGBM:\n",
    "    def __init__(self, X_test, y_test, metric):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.metric = metric\n",
    "\n",
    "    def __call__(self, env):\n",
    "        iteration = env.iteration + 1  # 0-indexed\n",
    "        y_pred = env.model.predict(self.X_test, num_iteration=iteration)\n",
    "\n",
    "        # 이진 분류인 경우 확률값 → 이진값 변환\n",
    "        if self.metric == f1_score:\n",
    "            y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "            score = self.metric(self.y_test, y_pred_binary)\n",
    "        else:\n",
    "            score = self.metric(self.y_test, y_pred)\n",
    "\n",
    "        metrics = {\"test\": score}\n",
    "\n",
    "        # 기존 validation loss 들도 같이 기록\n",
    "        for name, metric_name, value, _ in env.evaluation_result_list:\n",
    "            metrics[name] = value\n",
    "\n",
    "        mlflow.log_metrics(metrics, step=iteration, synchronous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "# MLflow 실험 시작\n",
    "mlflow.set_tracking_uri(\"http://175.214.62.133:50001/\")\n",
    "mlflow.set_experiment(\"lightgbm_rmsle_experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # 하이퍼파라미터 설정\n",
    "    params = {\n",
    "        'objective': 'binary',  # 회귀 문제\n",
    "        'metric': 'binary_logloss',           # 평가 지표\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "    }\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100),\n",
    "        MlflowLoggingCallbackLGBM(X_test, y_test, f1_score)\n",
    "    ]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data,valid_data],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=callbacks,\n",
    "        # feval = root_mean_squared_log_error_lgbm,\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # rmsle = root_mean_squared_log_error(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    print(\"f1:\", f1)\n",
    "\n",
    "    # MLflow에 파라미터 및 메트릭 기록\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    \n",
    "    # 모델 저장\n",
    "    mlflow.lightgbm.log_model(model, \"lightgbm_model\")\n",
    "    print(\"Model saved to MLflow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "template",
   "language": "python",
   "name": "template"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
